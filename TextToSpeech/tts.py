# The inference code is adopted from https://github.com/coqui-ai/TTS/blob/dev/TTS/demos/xtts_ft_demo/xtts_demo.py
# @title 2. ü§ó **S·ª≠ d·ª•ng**
# @markdown üëà Nh·∫•n ƒë·ªÉ ch·∫°y.
# @markdown N·∫øu g·∫∑p l·ªói th√¨ c≈©ng nh·∫•n n√∫t n√†y nh√©!

# @markdown L·∫ßn ƒë·∫ßu ch·∫°y s·∫Ω h∆°i l√¢u, b·∫°n ch·ªù t√≠ nh√©!

# @markdown K·∫øt qu·∫£ s·∫Ω ƒë∆∞·ª£c l∆∞u v√†o `/content/output`
# @markdown Ch·ªçn ng√¥n ng·ªØ:
#language = input("Nh·∫≠p lo·∫°i ng√¥n ng·ªØ(Ti·∫øng Vi·ªát, Ti·∫øng Anh, ...): ") # @param ["Ti·∫øng Vi·ªát", "Ti·∫øng Anh","Ti·∫øng T√¢y Ban Nha", "Ti·∫øng Ph√°p","Ti·∫øng ƒê·ª©c","Ti·∫øng √ù", "Ti·∫øng B·ªì ƒê√†o Nha", "Ti·∫øng Ba Lan", "Ti·∫øng Th·ªï Nhƒ© K·ª≥", "Ti·∫øng Nga", "Ti·∫øng H√† Lan", "Ti·∫øng S√©c", "Ti·∫øng ·∫¢ R·∫≠p", "Ti·∫øng Trung (gi·∫£n th·ªÉ)", "Ti·∫øng Nh·∫≠t", "Ti·∫øng Hungary", "Ti·∫øng H√†n", "Ti·∫øng Hindi"]
# @markdown VƒÉn b·∫£n ƒë·ªÉ ƒë·ªçc. ƒê·ªô d√†i t·ªëi thi·ªÉu m·ªói c√¢u n√™n t·ª´ 10 t·ª´ ƒë·ªÉ ƒë·∫∑t k·∫øt qu·∫£ t·ªët nh·∫•t.
#input_text =input("Nh·∫≠p vƒÉn b·∫£n: ") # @param {type:"string"}
# @markdown Ch·ªçn gi·ªçng m·∫´u:
#reference_audio = "model/st.mp3" # @param [ "model/user_sample.wav",  "model/vi_sample.wav",  "model/samples/nam-calm.wav",  "model/samples/nam-cham.wav",  "model/samples/nam-nhanh.wav",  "model/samples/nam-truyen-cam.wav",  "model/samples/nu-calm.wav",  "model/samples/nu-cham.wav",  "model/samples/nu-luu-loat.wav",  "model/samples/nu-nhan-nha.wav",  "model/samples/nu-nhe-nhang.wav"]
import argparse
parser = argparse.ArgumentParser(description="Text-to-Speech v·ªõi XTTS")
parser.add_argument('-language', type=str, required=True, help="Ng√¥n ng·ªØ (Ti·∫øng Vi·ªát, Ti·∫øng Anh, ...)")
parser.add_argument('-input', type=str, required=True, help="VƒÉn b·∫£n c·∫ßn ƒë·ªçc")
parser.add_argument('-reference', type=str, required=True, default="model/vi_sample.wav", help="ƒê∆∞·ªùng d·∫´n ƒë·∫øn file √¢m thanh tham chi·∫øu")
args = parser.parse_args()

language = args.language
input_text = args.input
reference_audio = args.reference
# @markdown T·ª± ƒë·ªông chu·∫©n h√≥a ch·ªØ (VD: 20/11 -> hai m∆∞∆°i th√°ng m∆∞·ªùi m·ªôt)
normalize_text = True # @param {type:"boolean"} #######
# @markdown In chi ti·∫øt x·ª≠ l√Ω
verbose = True # @param {type:"boolean"}
# @markdown L∆∞u t·ª´ng c√¢u th√†nh file ri√™ng l·∫ª.
output_chunks = False # @param {type:"boolean"}

from IPython.display import clear_output
def cry_and_quit():
    clear_output()
    print("> L·ªói r·ªìi huhu üò≠üò≠, b·∫°n h√£y nh·∫•n ch·∫°y l·∫°i ph·∫ßn n√†y nh√©!")
    quit()

import os
import string
import unicodedata
from datetime import datetime
from pprint import pprint

import torch
import torchaudio
from tqdm import tqdm
from underthesea import sent_tokenize
from unidecode import unidecode

try:
    from vinorm import TTSnorm
    from TTS.tts.configs.xtts_config import XttsConfig
    from TTS.tts.models.xtts import Xtts
except:
    cry_and_quit()

# Load model
def clear_gpu_cache():
    if torch.cuda.is_available():
        torch.cuda.empty_cache()


def load_model(xtts_checkpoint, xtts_config, xtts_vocab):
    clear_gpu_cache()
    if not xtts_checkpoint or not xtts_config or not xtts_vocab:
        return "You need to run the previous steps or manually set the `XTTS checkpoint path`, `XTTS config path`, and `XTTS vocab path` fields !!"
    config = XttsConfig()
    config.load_json(xtts_config)
    XTTS_MODEL = Xtts.init_from_config(config)
    print("Loading XTTS model! ")
    # The problem is likely caused by using deepspeed for inference in this context.
    # Disabling deepspeed for inference by setting use_deepspeed=False.
    XTTS_MODEL.load_checkpoint(config,
                               checkpoint_path=xtts_checkpoint,
                               vocab_path=xtts_vocab,
                               use_deepspeed=False) # Changed to False
    if torch.cuda.is_available():
        XTTS_MODEL.cuda()

    print("Model Loaded!")
    return XTTS_MODEL


# ... (rest of your code remains the same) ...


def get_file_name(text, max_char=50):
    filename = text[:max_char]
    filename = filename.lower()
    filename = filename.replace(" ", "_")
    filename = filename.translate(str.maketrans("", "", string.punctuation.replace("_", "")))
    filename = unidecode(filename)
    current_datetime = datetime.now().strftime("%m%d%H%M%S%f")
    filename = f"{current_datetime[:-3]}_{filename}"
    return filename


def calculate_keep_len(text, lang):
    if lang in ["ja", "zh-cn"]:
        return -1

    word_count = len(text.split())
    num_punct = (
        text.count(".")
        + text.count("!")
        + text.count("?")
        + text.count(",")
    )

    if word_count < 5:
        return 15000 * word_count + 2000 * num_punct
    elif word_count < 10:
        return 13000 * word_count + 2000 * num_punct
    return -1


def normalize_vietnamese_text(text):
    try:
        text = (
            TTSnorm(text, unknown=False, lower=False, rule=True)
            .replace("..", ".")
            .replace("!.", "!")
            .replace("?.", "?")
            .replace(" .", ".")
            .replace(" ,", ",")
            .replace('"', "")
            .replace("'", "")
            .replace("AI", "√Çy Ai")
            .replace("A.I", "√Çy Ai")
        )
        return text
    except Exception as e:
        print(f"L·ªói khi chu·∫©n h√≥a vƒÉn b·∫£n: {e}")
        return text  # Tr·∫£ v·ªÅ vƒÉn b·∫£n g·ªëc n·∫øu l·ªói


def run_tts(XTTS_MODEL, lang, tts_text, speaker_audio_file,
            normalize_text= True,
            verbose=False,
            output_chunks=False):
    """
    Run text-to-speech (TTS) synthesis using the provided XTTS_MODEL.

    Args:
        XTTS_MODEL: A pre-trained TTS model.
        lang (str): The language of the input text.
        tts_text (str): The text to be synthesized into speech.
        speaker_audio_file (str): Path to the audio file of the speaker to condition the synthesis on.
        normalize_text (bool, optional): Whether to normalize the input text. Defaults to True.
        verbose (bool, optional): Whether to print verbose information. Defaults to False.
        output_chunks (bool, optional): Whether to save synthesized speech chunks separately. Defaults to False.

    Returns:
        str: Path to the synthesized audio file.
    """

    if XTTS_MODEL is None or not speaker_audio_file:
        return "You need to run the previous step to load the model !!", None, None

    base_dir = os.path.dirname(os.path.abspath(__file__))
    # ƒêi l√™n th∆∞ m·ª•c cha (backend) r·ªìi sang frontend/mp3
    output_dir = os.path.join(base_dir, "..", "..", "frontend", "mp3")
    os.makedirs(output_dir, exist_ok=True)  # T·∫°o th∆∞ m·ª•c mp3 n·∫øu ch∆∞a t·ªìn t·∫°i

    gpt_cond_latent, speaker_embedding = XTTS_MODEL.get_conditioning_latents(
        audio_path=speaker_audio_file,
        gpt_cond_len=XTTS_MODEL.config.gpt_cond_len,
        max_ref_length=XTTS_MODEL.config.max_ref_len,
        sound_norm_refs=XTTS_MODEL.config.sound_norm_refs,
    )

    if normalize_text and lang == "vi":
        # Bug on google colab
        try:
            tts_text = normalize_vietnamese_text(tts_text)
        except:
            cry_and_quit()

    if lang in ["ja", "zh-cn"]:
        tts_texts = tts_text.split("„ÄÇ")
    else:
        tts_texts = sent_tokenize(tts_text)

    if verbose:
        print("Text for TTS:")
        pprint(tts_texts)

    wav_chunks = []
    for text in tqdm(tts_texts):
        if text.strip() == "":
            continue

        wav_chunk = XTTS_MODEL.inference(
            text=text,
            language=lang,
            gpt_cond_latent=gpt_cond_latent,
            speaker_embedding=speaker_embedding,
            temperature=0.3,
            length_penalty=1.0,
            repetition_penalty=10.0,
            top_k=30,
            top_p=0.85,
        )

        # Quick hack for short sentences
        keep_len = calculate_keep_len(text, lang)
        wav_chunk["wav"] = torch.tensor(wav_chunk["wav"][:keep_len])
        file_name = get_file_name(text)
        if output_chunks:
            out_path = os.path.join(output_dir, f"{file_name}.wav")
            torchaudio.save(out_path, wav_chunk["wav"].unsqueeze(0), 24000)
            if verbose:
                print(f"Saved chunk to mp3/{file_name}")

        wav_chunks.append(wav_chunk["wav"])

    out_wav = torch.cat(wav_chunks, dim=0).unsqueeze(0)
    out_path = os.path.join(output_dir, f"{file_name}.wav")
    torchaudio.save(out_path, out_wav, 24000)

    if verbose:
        print(f"Saved final file to mp3/{file_name}.wav")

    return f"mp3/{file_name}"


language_code_map = {
    "Ti·∫øng Vi·ªát": "vi",
    "Ti·∫øng Anh": "en",
    "Ti·∫øng T√¢y Ban Nha": "es",
    "Ti·∫øng Ph√°p": "fr",
    "Ti·∫øng ƒê·ª©c": "de",
    "Ti·∫øng √ù": "it",
    "Ti·∫øng B·ªì ƒê√†o Nha": "pt",
    "Ti·∫øng Ba Lan": "pl",
    "Ti·∫øng Th·ªï Nhƒ© K·ª≥": "tr",
    "Ti·∫øng Nga": "ru",
    "Ti·∫øng H√† Lan": "nl",
    "Ti·∫øng S√©c": "cs",
    "Ti·∫øng ·∫¢ R·∫≠p": "ar",
    "Ti·∫øng Trung (gi·∫£n th·ªÉ)": "zh-cn",
    "Ti·∫øng Nh·∫≠t": "ja",
    "Ti·∫øng Hungary": "hu",
    "Ti·∫øng H√†n": "ko",
    "Ti·∫øng Hindi": "hi"
}

print("> ƒêang n·∫°p m√¥ h√¨nh...")
try:
    if not vixtts_model:
        vixtts_model = load_model(xtts_checkpoint="model/model.pth",
                                xtts_config="model/config.json",
                                xtts_vocab="model/vocab.json")
except:
    vixtts_model = load_model(xtts_checkpoint="model/model.pth",
                                xtts_config="model/config.json",
                                xtts_vocab="model/vocab.json")
clear_output()
print("> ƒê√£ n·∫°p m√¥ h√¨nh")

if not os.path.exists(reference_audio):
    print("‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏èB·∫°n ch∆∞a t·∫£i file √¢m thanh l√™n. H√£y ch·ªçn gi·ªçng kh√°c, ho·∫∑c t·∫£i file c·ªßa b·∫°n l√™n ·ªü b√™n d∆∞·ªõi.‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è")
    audio_file="/content/model/vi_sample.wav"
else:
    audio_file = run_tts(vixtts_model,
            lang=language_code_map[language],
            tts_text=input_text,
            speaker_audio_file=reference_audio,
            normalize_text=normalize_text,
            verbose=verbose,
            output_chunks=output_chunks,)

#from IPython.display import Audio
#Audio(audio_file)